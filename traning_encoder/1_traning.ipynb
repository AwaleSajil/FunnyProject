{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rhome/sawale/FunnyProject/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    RobertaTokenizer, \n",
    "    RobertaForSequenceClassification, \n",
    "    RobertaConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import evaluate\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from transformers import TrainerCallback, DataCollatorWithPadding\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "MAX_LEN = 128\n",
    "TARGET_COLUMNS = ['humor', 'offensiveness', 'sentiment']\n",
    "# TARGET_COLUMNS = [ 'offensiveness', 'sentiment',  'humor']\n",
    "NUM_TARGETS = len(TARGET_COLUMNS)\n",
    "MODEL_CHECKPOINT = \"FacebookAI/roberta-base\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, focal_loss_gamma=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.focal_loss_gamma = focal_loss_gamma\n",
    "\n",
    "    def focal_binary_cross_entropy(self, logits, targets):\n",
    "        # Apply sigmoid to logits to get probabilities\n",
    "        p = torch.sigmoid(logits)\n",
    "        # Calculate focal loss\n",
    "        p_t = torch.where(targets == 1, p, 1 - p)\n",
    "        log_p_t = torch.log(torch.clamp(p_t, min=1e-4, max=1 - 1e-4))  # Cross Entropy\n",
    "        loss = - (1 - p_t) ** self.focal_loss_gamma * log_p_t\n",
    "        return loss.mean()\n",
    "\n",
    "    def compute_loss(self, model, inputs, num_items_in_batch=None, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # Compute focal loss\n",
    "        loss = self.focal_binary_cross_entropy(logits.view(-1), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback to record loss history\n",
    "class LossHistory(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.train_losses = []  # to store (global_step, training loss)\n",
    "        self.eval_losses = []   # to store (global_step, evaluation loss)\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        # Log training loss if available\n",
    "        if logs is not None and \"loss\" in logs:\n",
    "            self.train_losses.append((state.global_step, logs[\"loss\"]))\n",
    "    \n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        # Log evaluation loss if available\n",
    "        if metrics is not None and \"eval_loss\" in metrics:\n",
    "            self.eval_losses.append((state.global_step, metrics[\"eval_loss\"]))\n",
    "\n",
    "def plot_loss_history(loss_history, save_path=\"loss_plot.png\"):\n",
    "    \"\"\"\n",
    "    Plots the training and evaluation loss curves stored in the loss_history callback.\n",
    "    \"\"\"\n",
    "    if loss_history.train_losses:\n",
    "        train_steps, train_loss_values = zip(*loss_history.train_losses)\n",
    "    else:\n",
    "        train_steps, train_loss_values = [], []\n",
    "    \n",
    "    if loss_history.eval_losses:\n",
    "        eval_steps, eval_loss_values = zip(*loss_history.eval_losses)\n",
    "    else:\n",
    "        eval_steps, eval_loss_values = [], []\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_steps, train_loss_values, label=\"Training Loss\", marker='o')\n",
    "    plt.plot(eval_steps, eval_loss_values, label=\"Evaluation Loss\", marker='o')\n",
    "    plt.xlabel(\"Global Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Evaluation Loss Over Time\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    print(f\"Loss plot saved as '{save_path}'.\")\n",
    "\n",
    "# Function to load data from a parquet file and process targets\n",
    "def load_data(file_path, nrows=None):\n",
    "    # Load dataset from a Parquet file\n",
    "    df = pd.read_parquet(file_path)\n",
    "    if nrows:\n",
    "        df = df.head(nrows)\n",
    "    \n",
    "    # Cast the target columns to int for classification purposes.\n",
    "    df[TARGET_COLUMNS] = df[TARGET_COLUMNS].astype(int)\n",
    "    \n",
    "    # Drop rows where any value in the target columns isn't 0 or 1.\n",
    "    # This creates a boolean mask that checks for binary values.\n",
    "    df = df[df[TARGET_COLUMNS].isin([0, 1]).all(axis=1)]\n",
    "\n",
    "    df[TARGET_COLUMNS] = df[TARGET_COLUMNS].astype(float)\n",
    "    \n",
    "    # Ensure that the 'joke' column is of type string.\n",
    "    df['joke'] = df['joke'].astype(str)\n",
    "\n",
    "    # drop duplicates\n",
    "    df = df.drop_duplicates(subset=['joke'])\n",
    "    # drop empty jokes\n",
    "    df = df[df['joke'].str.strip() != '']\n",
    "    # shuffle the dataframe\n",
    "    df = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Custom dataset class for classification\n",
    "class JokeDataset(Dataset):\n",
    "    def __init__(self, jokes, targets, tokenizer, max_len):\n",
    "        self.jokes = jokes\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.jokes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        joke = str(self.jokes[idx])\n",
    "        # Convert target values to float for BCEWithLogitsLoss (they are binary: 0 or 1)\n",
    "        targets = np.array(self.targets[idx]).astype(np.float32)\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            joke,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(targets, dtype=torch.float)\n",
    "        }\n",
    "    \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Define the compute_metrics function for multi-label classification\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Apply sigmoid to get probabilities\n",
    "    sigmoid_preds = 1 / (1 + np.exp(-predictions))\n",
    "    # Threshold probabilities at 0.5 for binary predictions\n",
    "    binary_preds = (sigmoid_preds > 0.5).astype(int)\n",
    "    \n",
    "    # Compute precision, recall, and f1 scores for each target\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "    \n",
    "    for i in range(NUM_TARGETS):\n",
    "        precision = precision_score(labels[:, i], binary_preds[:, i], zero_division=0)\n",
    "        recall = recall_score(labels[:, i], binary_preds[:, i], zero_division=0)\n",
    "        f1 = f1_score(labels[:, i], binary_preds[:, i], zero_division=0)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "    \n",
    "    results = {\n",
    "        \"precision\": np.mean(precision_list),\n",
    "        \"recall\": np.mean(recall_list),\n",
    "        \"f1\": np.mean(f1_list)\n",
    "    }\n",
    "    for i, target in enumerate(TARGET_COLUMNS):\n",
    "        results[f\"precision_{target}\"] = precision_list[i]\n",
    "        results[f\"recall_{target}\"] = recall_list[i]\n",
    "        results[f\"f1_{target}\"] = f1_list[i]\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def iterative_split_dataframe(df, target_columns, input_columns=None, train_size=0.8, val_size=0.1, test_size=0.1):\n",
    "    \"\"\"\n",
    "    Splits a multi-label DataFrame into train, validation, and test sets using iterative stratification.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Full dataframe with both input features and target labels.\n",
    "        target_columns (list): List of column names that are the target labels.\n",
    "        input_columns (list, optional): List of input feature columns. If None, inferred from df.\n",
    "        train_size (float): Proportion of data for training.\n",
    "        val_size (float): Proportion of data for validation.\n",
    "        test_size (float): Proportion of data for testing.\n",
    "\n",
    "    Returns:\n",
    "        (train_df, val_df, test_df): Tuple of DataFrames.\n",
    "    \"\"\"\n",
    "    assert abs(train_size + val_size + test_size - 1.0) < 1e-6, \"Splits must sum to 1\"\n",
    "\n",
    "    if input_columns is None:\n",
    "        input_columns = [col for col in df.columns if col not in target_columns]\n",
    "\n",
    "    X = df[input_columns]\n",
    "    y = df[target_columns]\n",
    "\n",
    "    # First split: train and temp\n",
    "    temp_ratio = 1 - train_size\n",
    "    X_train, y_train, X_temp, y_temp = iterative_train_test_split(X.values, y.values, test_size=temp_ratio)\n",
    "\n",
    "    # Second split: val and test from temp\n",
    "    val_ratio = val_size / (val_size + test_size)\n",
    "    X_val, y_val, X_test, y_test = iterative_train_test_split(X_temp, y_temp, test_size=1 - val_ratio)\n",
    "\n",
    "    # Reconstruct DataFrames\n",
    "    train_df = pd.concat([pd.DataFrame(X_train, columns=input_columns),\n",
    "                          pd.DataFrame(y_train, columns=target_columns)], axis=1)\n",
    "    \n",
    "    val_df = pd.concat([pd.DataFrame(X_val, columns=input_columns),\n",
    "                        pd.DataFrame(y_val, columns=target_columns)], axis=1)\n",
    "\n",
    "    test_df = pd.concat([pd.DataFrame(X_test, columns=input_columns),\n",
    "                         pd.DataFrame(y_test, columns=target_columns)], axis=1)\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "def plot_probability_distributions(probabilities, labels, split_name, target_columns):\n",
    "    \"\"\"\n",
    "    Plots the distribution of predicted probabilities for each target.\n",
    "    \n",
    "    Args:\n",
    "        probabilities (np.ndarray): Array of shape (num_samples, num_targets) with probabilities.\n",
    "        labels (np.ndarray): Actual labels (not used in plot but could be overlaid).\n",
    "        split_name (str): Either \"Train\" or \"Test\" to label the plot.\n",
    "        target_columns (list): List of target column names.\n",
    "    \"\"\"\n",
    "    num_targets = len(target_columns)\n",
    "    plt.figure(figsize=(5 * num_targets, 5))\n",
    "\n",
    "    for i in range(num_targets):\n",
    "        plt.subplot(1, num_targets, i + 1)\n",
    "        sns.histplot(probabilities[:, i], bins=50, kde=True, color='skyblue')\n",
    "        plt.title(f\"{split_name} Set - {target_columns[i]}\")\n",
    "        plt.xlabel(\"Predicted Probability\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.xlim(0, 1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{split_name.lower()}_probability_distributions.png\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def main(data_path, nrows):\n",
    "    # Load data\n",
    "    print(f\"Loading data from {data_path}\")\n",
    "    df = load_data(data_path, nrows=nrows)\n",
    "\n",
    "    # Split data into train, validation, and test sets (80%, 10%, 10%)\n",
    "    train_df, val_df, test_df = iterative_split_dataframe(df, target_columns=TARGET_COLUMNS, input_columns=[\"joke\"], train_size=0.8, val_size=0.1, test_size=0.1)\n",
    "    \n",
    "    print(f\"Train set: {len(train_df)} samples\")\n",
    "    print(f\"Validation set: {len(val_df)} samples\")\n",
    "    print(f\"Test set: {len(test_df)} samples\")\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    input_to_encoding = lambda x: tokenizer(\n",
    "        list(x.values.flatten()), return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = JokeDataset(\n",
    "        jokes=train_df['joke'].values,\n",
    "        targets=train_df[TARGET_COLUMNS].values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "    \n",
    "    val_dataset = JokeDataset(\n",
    "        jokes=val_df['joke'].values,\n",
    "        targets=val_df[TARGET_COLUMNS].values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "    \n",
    "    test_dataset = JokeDataset(\n",
    "        jokes=test_df['joke'].values,\n",
    "        targets=test_df[TARGET_COLUMNS].values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "    \n",
    "    # Configure the model for multi-label classification\n",
    "    config = AutoConfig.from_pretrained(MODEL_CHECKPOINT)\n",
    "    config.num_labels = NUM_TARGETS\n",
    "    config.problem_type = \"multi_label_classification\"\n",
    "    \n",
    "    # Add id to label and label to id mappings to the model config\n",
    "    id2label = {i: label for i, label in enumerate(TARGET_COLUMNS)}\n",
    "    label2id = {label: i for i, label in enumerate(TARGET_COLUMNS)}\n",
    "    config.id2label = id2label\n",
    "    config.label2id = label2id\n",
    "    print(\"Mapping id to label:\", config.id2label)\n",
    "    print(\"Mapping label to id:\", config.label2id)\n",
    "    \n",
    "    # Initialize model; using AutoModelForSequenceClassification sets up BCEWithLogitsLoss internally.\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_CHECKPOINT,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    # Optionally, freeze the base model layers if you want to fine-tune only the classification head\n",
    "    for param in model.base_model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Instantiate loss history callback\n",
    "    loss_history = LossHistory()\n",
    "    \n",
    "    # Set up training arguments (note that we now use \"f1\" as our metric for best model)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=10,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=64,\n",
    "        warmup_steps=150,\n",
    "        weight_decay=0.1,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=150,\n",
    "        eval_steps=150,\n",
    "        save_steps=150,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_f1\",\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=2,\n",
    "        learning_rate=1.0e-5\n",
    "        # fp16=True,  # Uncomment if using mixed precision\n",
    "    )\n",
    "    \n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3), loss_history],\n",
    "        data_collator=data_collator,\n",
    "        focal_loss_gamma=1.5,\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    plot_loss_history(loss_history, save_path=\"loss_plot.png\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"Evaluating on test set...\")\n",
    "    test_results = trainer.evaluate(test_dataset)\n",
    "    print(\"Test results:\", test_results)\n",
    "    \n",
    "    # Save model\n",
    "    print(\"Saving model...\")\n",
    "    trainer.save_model(\"./joke_classification_model\")\n",
    "    \n",
    "    # Generate classification report for Train set\n",
    "    print(\"Generating classification report for the Train set...\")\n",
    "    train_predictions = trainer.predict(train_dataset)\n",
    "    train_logits = train_predictions.predictions\n",
    "    train_labels = train_predictions.label_ids\n",
    "    # Apply sigmoid and threshold to get binary predictions\n",
    "    train_sigmoid = 1 / (1 + np.exp(-train_logits))\n",
    "    train_binary_preds = (train_sigmoid > 0.5).astype(int)\n",
    "    \n",
    "    train_report = classification_report(train_labels, train_binary_preds, target_names=TARGET_COLUMNS, zero_division=0)\n",
    "    print(\"Train Classification Report:\")\n",
    "    print(train_report)\n",
    "    \n",
    "    # Generate classification report for Test set\n",
    "    print(\"Generating classification report for the Test set...\")\n",
    "    test_predictions = trainer.predict(test_dataset)\n",
    "    test_logits = test_predictions.predictions\n",
    "    test_labels = test_predictions.label_ids\n",
    "    test_sigmoid = 1 / (1 + np.exp(-test_logits))\n",
    "    test_binary_preds = (test_sigmoid > 0.5).astype(int)\n",
    "    \n",
    "    test_report = classification_report(test_labels, test_binary_preds, target_names=TARGET_COLUMNS, zero_division=0)\n",
    "    print(\"Test Classification Report:\")\n",
    "    print(test_report)\n",
    "    \n",
    "    # (Optional) Save predictions/actuals for further analysis\n",
    "    print(\"Sample binary predictions (first 5):\", test_binary_preds[:5])\n",
    "\n",
    "    plot_probability_distributions(train_sigmoid, train_labels, \"Train\", TARGET_COLUMNS)\n",
    "    plot_probability_distributions(test_sigmoid, test_labels, \"Test\", TARGET_COLUMNS)\n",
    "\n",
    "\n",
    "    \n",
    "    # Return results for further use if needed\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"test_results\": test_results,\n",
    "        \"train_report\": train_report,\n",
    "        \"test_report\": test_report,\n",
    "        \"raw_predictions\": test_logits,\n",
    "        \"binary_predictions\": test_binary_preds,\n",
    "        \"actual_values\": test_labels\n",
    "    }\n",
    "\n",
    "\n",
    "def predict_joke_ratings(joke_text, model_path=\"./joke_classification_model\"):\n",
    "    \"\"\"\n",
    "    Use the trained classification model to predict ratings for a new joke.\n",
    "    \n",
    "    Args:\n",
    "        joke_text (str): The text of the joke to rate.\n",
    "        model_path (str): Path to the saved model.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Predicted ratings (0/1) for each metric, with labels from the model configuration.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "    # Load the model configuration and extract the id2label mapping\n",
    "    config = AutoConfig.from_pretrained(model_path)\n",
    "    id2label = config.id2label  # id2label should be a dict with integer keys mapping to target labels\n",
    "    \n",
    "    # Load the model using the updated configuration\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path, config=config)\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize the joke text\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        joke_text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=encoding['input_ids'],\n",
    "            attention_mask=encoding['attention_mask']\n",
    "        )\n",
    "        logits = outputs.logits.cpu().numpy()[0]\n",
    "    \n",
    "    # Convert logits to probabilities and then to binary predictions\n",
    "    sigmoid_probs = 1 / (1 + np.exp(-logits))\n",
    "    binary_preds = (sigmoid_probs > 0.5).astype(int)\n",
    "    \n",
    "    # Use the id2label mapping from the model configuration to form the results\n",
    "    results = {id2label[i]: int(pred) for i, pred in enumerate(binary_preds)}\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../data/labeled_jokes_classification_mistral:latest.parquet\n",
      "Train set: 44220 samples\n",
      "Validation set: 5531 samples\n",
      "Test set: 5527 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping id to label: {0: 'humor', 1: 'offensiveness', 2: 'sentiment'}\n",
      "Mapping label to id: {'humor': 0, 'offensiveness': 1, 'sentiment': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rhome/sawale/FunnyProject/.venv/lib/python3.11/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rhome/sawale/FunnyProject/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='991' max='6910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 991/6910 15:19 < 1:31:44, 1.08 it/s, Epoch 1.43/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision Humor</th>\n",
       "      <th>Recall Humor</th>\n",
       "      <th>F1 Humor</th>\n",
       "      <th>Precision Offensiveness</th>\n",
       "      <th>Recall Offensiveness</th>\n",
       "      <th>F1 Offensiveness</th>\n",
       "      <th>Precision Sentiment</th>\n",
       "      <th>Recall Sentiment</th>\n",
       "      <th>F1 Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.199500</td>\n",
       "      <td>0.167206</td>\n",
       "      <td>0.777563</td>\n",
       "      <td>0.735573</td>\n",
       "      <td>0.703631</td>\n",
       "      <td>0.730140</td>\n",
       "      <td>0.972178</td>\n",
       "      <td>0.833952</td>\n",
       "      <td>0.751708</td>\n",
       "      <td>0.234542</td>\n",
       "      <td>0.357530</td>\n",
       "      <td>0.850841</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.919410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.156600</td>\n",
       "      <td>0.146405</td>\n",
       "      <td>0.811174</td>\n",
       "      <td>0.786280</td>\n",
       "      <td>0.784711</td>\n",
       "      <td>0.790040</td>\n",
       "      <td>0.924409</td>\n",
       "      <td>0.851959</td>\n",
       "      <td>0.756944</td>\n",
       "      <td>0.464819</td>\n",
       "      <td>0.575958</td>\n",
       "      <td>0.886536</td>\n",
       "      <td>0.969613</td>\n",
       "      <td>0.926215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.148700</td>\n",
       "      <td>0.141272</td>\n",
       "      <td>0.809228</td>\n",
       "      <td>0.806509</td>\n",
       "      <td>0.800403</td>\n",
       "      <td>0.802804</td>\n",
       "      <td>0.916798</td>\n",
       "      <td>0.856023</td>\n",
       "      <td>0.737673</td>\n",
       "      <td>0.531628</td>\n",
       "      <td>0.617926</td>\n",
       "      <td>0.887206</td>\n",
       "      <td>0.971101</td>\n",
       "      <td>0.927260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>0.139205</td>\n",
       "      <td>0.813030</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>0.811711</td>\n",
       "      <td>0.816149</td>\n",
       "      <td>0.909974</td>\n",
       "      <td>0.860511</td>\n",
       "      <td>0.712479</td>\n",
       "      <td>0.600569</td>\n",
       "      <td>0.651755</td>\n",
       "      <td>0.910463</td>\n",
       "      <td>0.935614</td>\n",
       "      <td>0.922867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.135600</td>\n",
       "      <td>0.139230</td>\n",
       "      <td>0.807405</td>\n",
       "      <td>0.829397</td>\n",
       "      <td>0.818184</td>\n",
       "      <td>0.836282</td>\n",
       "      <td>0.880840</td>\n",
       "      <td>0.857983</td>\n",
       "      <td>0.670175</td>\n",
       "      <td>0.678749</td>\n",
       "      <td>0.674435</td>\n",
       "      <td>0.915759</td>\n",
       "      <td>0.928602</td>\n",
       "      <td>0.922135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.130200</td>\n",
       "      <td>0.134559</td>\n",
       "      <td>0.827435</td>\n",
       "      <td>0.815725</td>\n",
       "      <td>0.815741</td>\n",
       "      <td>0.824666</td>\n",
       "      <td>0.907349</td>\n",
       "      <td>0.864034</td>\n",
       "      <td>0.760952</td>\n",
       "      <td>0.567875</td>\n",
       "      <td>0.650387</td>\n",
       "      <td>0.896687</td>\n",
       "      <td>0.971951</td>\n",
       "      <td>0.932803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rhome/sawale/FunnyProject/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/rhome/sawale/FunnyProject/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/rhome/sawale/FunnyProject/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/rhome/sawale/FunnyProject/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/rhome/sawale/FunnyProject/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/rhome/sawale/FunnyProject/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Replace with your actual file path\n",
    "data_path = \"../data/labeled_jokes_classification_mistral:latest.parquet\"\n",
    "results = main(data_path, nrows=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using the model with a new joke\n",
    "print(\"\\nExample prediction:\")\n",
    "sample_joke = \"Why don't scientists trust atoms? Because they make up everything!\"\n",
    "predicted_ratings = predict_joke_ratings(sample_joke)\n",
    "for dimension, rating in predicted_ratings.items():\n",
    "    print(f\"{dimension}: {rating}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using the model with a new joke\n",
    "print(\"\\nExample prediction:\")\n",
    "sample_joke = \"Mother\"\n",
    "predictions = predict_joke_ratings(sample_joke)\n",
    "for dimension, score in predictions.items():\n",
    "    print(f\"{dimension}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
