{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sawale/Documents/FunnyProject/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    RobertaTokenizer, \n",
    "    RobertaForSequenceClassification, \n",
    "    RobertaConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import evaluate\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "MAX_LEN = 512\n",
    "TARGET_COLUMNS = ['humor', 'offensiveness', 'clarity', 'surprise_factor', \n",
    "                 'relatability', 'novelty', 'conciseness', 'sentiment']\n",
    "NUM_TARGETS = len(TARGET_COLUMNS)\n",
    "MODEL_CHECKPOINT = \"distilbert/distilroberta-base\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "def load_data(file_path, nrows=None):\n",
    "    # Load dataset from a Parquet file\n",
    "    df = pd.read_parquet(file_path)\n",
    "    if nrows:\n",
    "        df = df.head(nrows)\n",
    "    \n",
    "    # Define the numerical columns:\n",
    "    # Non-sentiment numerical columns\n",
    "    non_sentiment_columns = [\n",
    "        'humor', 'offensiveness', 'clarity', \n",
    "        'surprise_factor', 'relatability', 'novelty', \n",
    "        'conciseness'\n",
    "    ]\n",
    "    # Sentiment column as a separate list\n",
    "    sentiment_column = ['sentiment']\n",
    "\n",
    "    # First, apply the MinMaxScaler to the non-sentiment columns with a range of 0 to 100.\n",
    "    minmax_scaler_non_sent = MinMaxScaler(feature_range=(0, 100))\n",
    "    df[non_sentiment_columns] = minmax_scaler_non_sent.fit_transform(df[non_sentiment_columns])\n",
    "    \n",
    "    # Then, apply the MinMaxScaler to the sentiment column with a range of -100 to 100.\n",
    "    minmax_scaler_sent = MinMaxScaler(feature_range=(-100, 100))\n",
    "    df[sentiment_column] = minmax_scaler_sent.fit_transform(df[sentiment_column])\n",
    "    \n",
    "    # Combine all numerical columns for further scaling\n",
    "    numerical_columns = non_sentiment_columns + sentiment_column\n",
    "    \n",
    "    # Now, apply StandardScaler to standardize (zero mean, unit variance) the already min-max scaled columns.\n",
    "    standard_scaler = StandardScaler()\n",
    "    df[numerical_columns] = standard_scaler.fit_transform(df[numerical_columns])\n",
    "    \n",
    "    return df, standard_scaler\n",
    "\n",
    "# Custom dataset class\n",
    "class JokeDataset(Dataset):\n",
    "    def __init__(self, jokes, targets, tokenizer, max_len):\n",
    "        self.jokes = jokes\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.jokes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        joke = str(self.jokes[idx])\n",
    "        targets = self.targets[idx].astype(np.float32)\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            joke,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            # 'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'labels': torch.tensor(targets, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "# Define a custom model class for multiple regression\n",
    "class RobertaForMultipleRegression(RobertaForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n",
    "                position_ids=None, head_mask=None, inputs_embeds=None, labels=None,\n",
    "                output_attentions=None, output_hidden_states=None, return_dict=None):\n",
    "        \n",
    "        return super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            # token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            labels=labels,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict\n",
    "        )\n",
    "    \n",
    "\n",
    "# Define the compute_metrics function for Trainer\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Calculate MSE for each target dimension\n",
    "    mse_per_target = []\n",
    "    for i in range(NUM_TARGETS):\n",
    "        mse = mean_squared_error(labels[:, i], predictions[:, i])\n",
    "        mse_per_target.append(mse)\n",
    "    \n",
    "    # Calculate average MSE across all targets\n",
    "    avg_mse = np.mean(mse_per_target)\n",
    "    \n",
    "    # Create result dictionary with individual and average MSE\n",
    "    results = {\"mse\": avg_mse}\n",
    "    for i, target in enumerate(TARGET_COLUMNS):\n",
    "        results[f\"mse_{target}\"] = mse_per_target[i]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Main function\n",
    "def main(data_path, nrows):\n",
    "    # Load data\n",
    "    print(f\"Loading data from {data_path}\")\n",
    "    df, scalar = load_data(data_path, nrows)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Split data into train, validation, and test sets (80%, 10%, 10%)\n",
    "    train_df, temp_df = train_test_split(df, test_size=0.2, random_state=SEED)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=SEED)\n",
    "    \n",
    "    print(f\"Train set: {len(train_df)} samples\")\n",
    "    print(f\"Validation set: {len(val_df)} samples\")\n",
    "    print(f\"Test set: {len(test_df)} samples\")\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = JokeDataset(\n",
    "        jokes=train_df['joke'].values,\n",
    "        targets=train_df[TARGET_COLUMNS].values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "    \n",
    "    val_dataset = JokeDataset(\n",
    "        jokes=val_df['joke'].values,\n",
    "        targets=val_df[TARGET_COLUMNS].values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "    \n",
    "    test_dataset = JokeDataset(\n",
    "        jokes=test_df['joke'].values,\n",
    "        targets=test_df[TARGET_COLUMNS].values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "    \n",
    "    # Configure the model for regression task\n",
    "    config = AutoConfig.from_pretrained(MODEL_CHECKPOINT)\n",
    "    config.num_labels = NUM_TARGETS\n",
    "    config.problem_type = \"regression\"\n",
    "    \n",
    "    # Initialize model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_CHECKPOINT,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    # Set up training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=10,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=64,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_mse\",\n",
    "        greater_is_better=False,\n",
    "        save_total_limit=2,\n",
    "        learning_rate=5.0e-5\n",
    "        # fp16=True,  # Use mixed precision training if available\n",
    "    )\n",
    "    \n",
    "    # Set up trainer with early stopping\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"Evaluating on test set...\")\n",
    "    test_results = trainer.evaluate(test_dataset)\n",
    "    print(\"Test results:\", test_results)\n",
    "    \n",
    "    # Save model\n",
    "    print(\"Saving model...\")\n",
    "    trainer.save_model(\"./joke_regression_model\")\n",
    "    \n",
    "    # Perform predictions on test set (for further analysis if needed)\n",
    "    test_predictions = trainer.predict(test_dataset)\n",
    "    predictions = test_predictions.predictions\n",
    "    actual_values = test_predictions.label_ids\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"test_results\": test_results,\n",
    "        \"predictions\": predictions,\n",
    "        \"actual_values\": actual_values\n",
    "    }\n",
    "\n",
    "def predict_joke_ratings(joke_text, model_path=\"./joke_regression_model\"):\n",
    "    \"\"\"\n",
    "    Use the trained model to predict ratings for a new joke.\n",
    "    \n",
    "    Args:\n",
    "        joke_text (str): The text of the joke to rate\n",
    "        model_path (str): Path to the saved model\n",
    "    \n",
    "    Returns:\n",
    "        dict: Predicted ratings for each dimension\n",
    "    \"\"\"\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "    config = AutoConfig.from_pretrained(model_path)\n",
    "    model = RobertaForMultipleRegression.from_pretrained(model_path, config=config)\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        joke_text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Perform prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=encoding['input_ids'],\n",
    "            attention_mask=encoding['attention_mask'],\n",
    "            # token_type_ids=encoding['token_type_ids']\n",
    "        )\n",
    "        predictions = outputs.logits.cpu().numpy()[0]\n",
    "    \n",
    "    # Format results\n",
    "    result = {}\n",
    "    for i, target in enumerate(TARGET_COLUMNS):\n",
    "        result[target] = float(predictions[i])\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Function to analyze model performance\n",
    "def analyze_results(predictions, actual_values):\n",
    "    \"\"\"\n",
    "    Analyze the model's performance on each target dimension.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Model predictions\n",
    "        actual_values: Ground truth values\n",
    "        \n",
    "    Returns:\n",
    "        dict: Performance metrics\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # For each target dimension\n",
    "    for i, target in enumerate(TARGET_COLUMNS):\n",
    "        # Calculate MSE\n",
    "        mse = mean_squared_error(actual_values[:, i], predictions[:, i])\n",
    "        \n",
    "        # Calculate correlation\n",
    "        corr = np.corrcoef(predictions[:, i], actual_values[:, i])[0, 1]\n",
    "        \n",
    "        results[target] = {\n",
    "            'mse': mse,\n",
    "            'correlation': corr\n",
    "        }\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    results['overall'] = {\n",
    "        'mse': mean_squared_error(actual_values.flatten(), predictions.flatten()),\n",
    "        'correlation': np.corrcoef(predictions.flatten(), actual_values.flatten())[0, 1]\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../data/labeled_jokes.parquet\n",
      "Train set: 8000 samples\n",
      "Validation set: 1000 samples\n",
      "Test set: 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilbert/distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/sawale/Documents/FunnyProject/.venv/lib/python3.11/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='216' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 216/5000 05:25 < 2:01:11, 0.66 it/s, Epoch 0.43/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Replace with your actual file path\n",
    "data_path = \"../data/labeled_jokes.parquet\"\n",
    "results = main(data_path, nrows=10000)\n",
    "\n",
    "# Analyze performance\n",
    "print(\"\\nAnalyzing model performance:\")\n",
    "performance = analyze_results(results[\"predictions\"], results[\"actual_values\"])\n",
    "for dimension, metrics in performance.items():\n",
    "    if dimension != 'overall':\n",
    "        print(f\"{dimension}: MSE = {metrics['mse']:.4f}, Correlation = {metrics['correlation']:.4f}\")\n",
    "print(f\"Overall: MSE = {performance['overall']['mse']:.4f}, Correlation = {performance['overall']['correlation']:.4f}\")\n",
    "\n",
    "# Example of using the model with a new joke\n",
    "print(\"\\nExample prediction:\")\n",
    "sample_joke = \"Why don't scientists trust atoms? Because they make up everything!\"\n",
    "predictions = predict_joke_ratings(sample_joke)\n",
    "for dimension, score in predictions.items():\n",
    "    print(f\"{dimension}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using the model with a new joke\n",
    "print(\"\\nExample prediction:\")\n",
    "sample_joke = \"Negga\"\n",
    "predictions = predict_joke_ratings(sample_joke)\n",
    "for dimension, score in predictions.items():\n",
    "    print(f\"{dimension}: {score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
